{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the NYC Subway Dataset\n",
    "\n",
    "Project connected to the [Udacity Intro to Data Science course](https://www.udacity.com/course/viewer#!/c-ud359-nd).\n",
    "\n",
    "by Victor Ribeiro, October/2015\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0. References\n",
    "\n",
    "**About the Dataset**\n",
    "\n",
    "Turnstile and Weather Variables dataset reports on the cumulative number of entries and exists in the NYC with additional information about the weather. \n",
    "\n",
    "* [Original Dataset](https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv) - data set used throughout the course and used in the report below.\n",
    "* [Improved Dataset](https://www.dropbox.com/s/1lpoeh2w6px4diu/improved-dataset.zip?dl=0) - cleaned-up subset of original dataset with additional variables. [Variables in the dataset](https://s3.amazonaws.com/uploads.hipchat.com/23756/665149/05bgLZqSsMycnkg/turnstile-weather-variables.pdf)\n",
    "\n",
    "**References**\n",
    "\n",
    "* [Mann-Whitney U Test](https://storage.googleapis.com/supplemental_media/udacityu/4332539257/MannWhitneyUTest.pdf) Udacity\n",
    "* [Mann-Whitney U Test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) Wikipedia\n",
    "* [Shapiro-Wilk Test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) Wikipedia\n",
    "* [Shapiro-Wild Test](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html) Python reference\n",
    "* [Diez, David; Barr, Christopher; Çetinkaya-Rundel, Mine] OpenIntro Statistics, Third Edition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1. Statistical Test\n",
    "\n",
    "**1.1 Which statistical test did you use to analyze the NYC subway data? Did you use a one-tail or a two-tail P value? What is the null hypothesis? What is your p-critical value?**\n",
    "\n",
    ">Considering the proposed project :\n",
    ">* Null hypothesis : there's no difference between number of rides in the metro while raining vs. no raining. \n",
    ">* Alternative hypothesis : there's a difference between number of rides in the metro while raining  vs.\n",
    "no raining.\n",
    ">\n",
    ">A Mann-Whitney U Test is applied. It's a two-tail test and p-critical value is 5% (or 0.05).\n",
    "\n",
    "---\n",
    "**1.2 Why is this statistical test applicable to the dataset? In particular, consider the assumptions that the test is making about the distribution of ridership in the two samples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Taking a look in the data to support decision on 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandasql\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import csv\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from ggplot import *\n",
    "import itertools\n",
    "\n",
    "df = pandas.read_csv(\"turnstile_data_master_with_weather.csv\")\n",
    "#df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bins = 25\n",
    "alpha = 0.75\n",
    "df[df['rain']==0]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "df[df['rain']==1]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "    \n",
    "plt.suptitle('Histogram of ENTRIESn_hourly')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('ENTRIESn_hourly')\n",
    "plt.legend(['no rain', 'rain'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram Raining and Non-Raining](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As per histogram above neither raining nor no-raining data follow a normal distribution. Indeed, by applying Shapiro-Wik test (below), we confirm datasets do not follow normal distribution as p-value for shapiro test on both raining / no-raining data is really small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scipy.stats.shapiro(df[df['rain']==0]['ENTRIESn_hourly'])\n",
    "print scipy.stats.shapiro(df[df['rain']==1]['ENTRIESn_hourly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Thus, a non-parametric test (a test that does not assume the data is drawn from any particular underlying probability distribution) like Mann-Whithney U Test is applicable.\n",
    "\n",
    "---\n",
    "**1.3 What results did you get from this statistical test? These should include the following numerical values: p-values, as well as the means for each of the two samples under test.**\n",
    "\n",
    ">Applying Mann-Whitney U Test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with_rain_mean = np.mean(df[df['rain']==1]['ENTRIESn_hourly'])\n",
    "without_rain_mean = np.mean(df[df['rain']==0]['ENTRIESn_hourly'])\n",
    "    \n",
    "U,p = scipy.stats.mannwhitneyu(df[df['rain']==1]['ENTRIESn_hourly'],\n",
    "                               df[df['rain']==0]['ENTRIESn_hourly'])\n",
    "\n",
    "print 'Mean with rain :',with_rain_mean, '\\nMean without rain :', without_rain_mean, '\\nU :', U, '\\n2*p:', 2*p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 What is the significance and interpretation of these results?**\n",
    "\n",
    ">`ENTRIESn_hourly` raining mean is slightly bigger than no-raining means. 2 \\* p-value is slightly below 0.05, thus **null hypothesis is rejected**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2. Linear Regression\n",
    "\n",
    "**2.1 What approach did you use to compute the coefficients theta and produce prediction for ENTRIESn_hourly in your regression model:**\n",
    "- **OLS using Statsmodels or Scikit Learn,**\n",
    "- **Gradient descent using Scikit Learn,**\n",
    "- **Or something different?**\n",
    "\n",
    ">OLS (using Statsmodels) has been selected to predict `ENTRIESn_hourly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_regression(features, values):\n",
    "    model = sm.OLS(values,sm.add_constant(features))\n",
    "    results = model.fit()\n",
    "    intercept = results.params[0]\n",
    "    params = results.params[1:]    \n",
    "    \n",
    "    return intercept, params\n",
    "\n",
    "features = df[['Hour','rain','meantempi','minpressurei']] \n",
    "dummy_units = pandas.get_dummies(df['UNIT'], prefix='unit')\n",
    "features = features.join(dummy_units)\n",
    "\n",
    "# Perform linear regression\n",
    "intercept, params = linear_regression(features, df['ENTRIESn_hourly'])\n",
    "    \n",
    "predictions = intercept + np.dot(features, params)\n",
    "\n",
    "def compute_r_squared(data, predictions):\n",
    "    n = ((data - predictions)**2).sum()\n",
    "    d = ((data - data.mean())**2).sum()\n",
    "    \n",
    "    r_squared = 1 - n/d\n",
    "    return r_squared\n",
    "\n",
    "compute_r_squared(df['ENTRIESn_hourly'], predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Trying some brute force to chose features after failing miserably with gut feeling only (different sets leaded to very different values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# collection with almost all features. Following Ex 5 on Set 3, EXITSn_hourly is not being used.\n",
    "aall_features = ['rain','fog', 'precipi', 'Hour', \n",
    "                 'meanwindspdi',\n",
    "                 'meantempi', \n",
    "                 'meanpressurei',\n",
    "                 'meandewpti' ]\n",
    "\n",
    "                #'maxtempi', 'mintempi', 'meantempi', \n",
    "                #'maxpressurei', 'minpressurei', 'meanpressurei',\n",
    "                #'maxdewpti', 'mindewpti', 'meandewpti' ]\n",
    "\n",
    "# multiple variables to log results\n",
    "i = 0\n",
    "t_feat = []\n",
    "t_inte = []\n",
    "t_para = []\n",
    "t_pred = []\n",
    "t_rsqu = []\n",
    "t_subs = []\n",
    "log_experiments = []\n",
    "\n",
    "# global max logs and counter\n",
    "r_max = -1\n",
    "s_max = 'none'\n",
    "j = 0\n",
    "\n",
    "# This brute force loop will select all combinations of some specific sizes. \n",
    "# At first, not using dummies variables.\n",
    "for L in range(3,(len(aall_features)+1)):\n",
    "    log_experiments.append(j)\n",
    "    r_max_local = -1\n",
    "    \n",
    "    # for each combination, runs a linear regression, loging data, preserving max\n",
    "    for subset in itertools.combinations(aall_features, L):\n",
    "        t_feat.append(i)\n",
    "        t_inte.append(i)\n",
    "        t_para.append(i)\n",
    "        t_pred.append(i)\n",
    "        t_rsqu.append(i)\n",
    "        t_subs.append(i)\n",
    "        \n",
    "        t_subs[i] = subset\n",
    "        \n",
    "        # Selected features\n",
    "        t_feat[i] = df[[subset[0]]]\n",
    "        for k in range(1,len(subset)):\n",
    "            t_feat[i] = t_feat[i].join(df[[subset[k]]])\n",
    "        \n",
    "        # dummy variable\n",
    "        t_feat[i] = t_feat[i].join(pandas.get_dummies(df['UNIT'], prefix='unit'))\n",
    "\n",
    "        # Perform linear regression\n",
    "        t_inte[i], t_para[i] = linear_regression(t_feat[i], df['ENTRIESn_hourly'])\n",
    "        t_pred[i] = t_inte[i] + np.dot(t_feat[i], t_para[i])\n",
    "        t_rsqu[i] = compute_r_squared(df['ENTRIESn_hourly'], t_pred[i])\n",
    "        \n",
    "        # Saving max for each combination size\n",
    "        if r_max_local < t_rsqu[i]:\n",
    "            r_max_local = t_rsqu[i]\n",
    "            log_experiments[j] = [r_max_local, t_subs[i], i]\n",
    "            \n",
    "        #print 'Test ',i,' Subset:', subset, 'R2:', t_rsqu[i]\n",
    "        i = i+1\n",
    "    \n",
    "    # Saving total max for all combinations size so far\n",
    "    if r_max < r_max_local:\n",
    "        r_max = r_max_local\n",
    "        s_max = log_experiments[j]\n",
    "        \n",
    "    j = j+1\n",
    "        \n",
    "# print Rˆ2 max for each combination size and features used\n",
    "for k in range(0,len(log_experiments)):\n",
    "    print log_experiments[k][0], log_experiments[k][1], log_experiments[k][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now for the best combinations without dummies, add dummies variables\n",
    "log_experiments_dummies = []\n",
    "\n",
    "j = 0\n",
    "for k in log_experiments:\n",
    "    log_experiments_dummies.append(j)\n",
    "    \n",
    "    subset = log_experiments[j][1]\n",
    "    \n",
    "    t_feat.append(i)\n",
    "    t_inte.append(i)\n",
    "    t_para.append(i)\n",
    "    t_pred.append(i)\n",
    "    t_rsqu.append(i)\n",
    "    t_subs.append(i)\n",
    "        \n",
    "    t_subs[i] = subset\n",
    "        \n",
    "    # Selected features + dummies\n",
    "    t_feat[i] = df[[subset[0]]]\n",
    "    for k in range(1,len(subset)):\n",
    "        t_feat[i] = t_feat[i].join(df[[subset[k]]])\n",
    "            \n",
    "    t_feat[i] = t_feat[i].join(pandas.get_dummies(df['UNIT'], prefix='unit'))\n",
    "        \n",
    "    # Perform linear regression\n",
    "    t_inte[i], t_para[i] = linear_regression(t_feat[i], df['ENTRIESn_hourly'])\n",
    "    t_pred[i] = t_inte[i] + np.dot(t_feat[i], t_para[i])\n",
    "    t_rsqu[i] = compute_r_squared(df['ENTRIESn_hourly'], t_pred[i])\n",
    "      \n",
    "    log_experiments_dummies[j] = [t_rsqu[i], t_subs[i], i]\n",
    "            \n",
    "    # Saving global max so far\n",
    "    if r_max < t_rsqu[i]:\n",
    "        r_max = t_rsqu[i]\n",
    "        s_max = log_experiments_dummies[j]    \n",
    "\n",
    "    #print 'Test ',i,' Subset:', subset, 'R2:', t_rsqu[i]\n",
    "    i = i+1        \n",
    "    j = j+1\n",
    "\n",
    "    \n",
    "# print Rˆ2 max for each subset with dummies and features used\n",
    "for k in range(0,len(log_experiments_dummies)):\n",
    "    print log_experiments_dummies[k][0], log_experiments_dummies[k][1], '+ Dummies : UNIT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**2.2 What features (input variables) did you use in your model? Did you use any dummy variables as part of your features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Selected variables were : UNIT as dummy variable and :', s_max[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**2.3 Why did you select these features in your model? We are looking for specific reasons that lead you to believe that the selected features will contribute to the predictive power of your model.**\n",
    "- **Your reasons might be based on intuition. For example, response for fog might be: “I decided to use fog because I thought that when it is very foggy outside people might decide to use the subway more often.”**\n",
    "- **Your reasons might also be based on data exploration and experimentation, for example: “I used feature X because as soon as I included it in my model, it drastically improved my R2 value.”**\n",
    "\n",
    ">As I lived majority of my live in a city without metro, I do not have any strong gut feeling about what are the variables that would influentiated the most. Indeed, I've tried different combinations and results were really different one from each other. Thus, I've decided for some brute force : (i) pick all combinations with 4 and 5 elements and calculate R2 without a dummy variable and then, pick the best results and try with a dummy variable.\n",
    ">\n",
    ">The dummy variable 'UNIT' drastically improves R2 value. Max R2 value for features mentioned in 2.2 is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print r_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.4 What are the parameters (also known as \"coefficients\" or \"weights\") of the non-dummy features in your linear regression model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_para[s_max[2]].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**2.5 What is your model’s R2 (coefficients of determination) value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compute_r_squared(df['ENTRIESn_hourly'], t_pred[s_max[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.6 What does this R2 value mean for the goodness of fit for your regression model? Do you think this linear model to predict ridership is appropriate for this dataset, given this R2  value?**\n",
    "\n",
    "> R2 is the the percentage of variance that is explained. The closer R2 is to one, the better is the model. And, the closer to zero, the worse is the model. Our R2 is smaller than 0.5 (closer to zero than to one) which is mid-term, not good, but not bad. Below, a histogram of residuals (original data - predicted data) is presented. Most of the residuals are close to zero +/- 5000.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('Histogram of residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Difference original vs predicted')\n",
    "plt.grid(True)\n",
    "(df['ENTRIESn_hourly'] - t_pred[s_max[2]]).hist(bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram of residuals](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3. Visualization\n",
    "\n",
    "**Please include two visualizations that show the relationships between two or more variables in the NYC subway data.\n",
    "Remember to add appropriate titles and axes labels to your plots. Also, please add a short description below each figure commenting on the key insights depicted in the figure.**\n",
    "\n",
    "**3.1 One visualization should contain two histograms: one of  ENTRIESn_hourly for rainy days and one of ENTRIESn_hourly for non-rainy days.**\n",
    "- **You can combine the two histograms in a single plot or you can use two separate plots.**\n",
    "- **If you decide to use to two separate plots for the two histograms, please ensure that the x-axis limits for both of the plots are identical. It is much easier to compare the two in that case.**\n",
    "- **For the histograms, you should have intervals representing the volume of ridership (value of ENTRIESn_hourly) on the x-axis and the frequency of occurrence on the y-axis. For example, each interval (along the x-axis), the height of the bar for this interval will represent the number of records (rows in our data) that have ENTRIESn_hourly that falls in this interval.**\n",
    "- **Remember to increase the number of bins in the histogram (by having larger number of bars). The default bin width is not sufficient to capture the variability in the two samples.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bins = 20\n",
    "alpha = 0.50\n",
    "df[df['rain']==0]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "df[df['rain']==1]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "    \n",
    "plt.suptitle('Histogram of ENTRIESn_hourly')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('ENTRIESn_hourly')\n",
    "plt.legend(['no rain', 'rain'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram Raining and Non-Raining](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**3.2 One visualization can be more freeform. You should feel free to implement something that we discussed in class (e.g., scatter plots, line plots) or attempt to implement something more advanced if you'd like. Some suggestions are:**\n",
    "- **Ridership by time-of-day**\n",
    "- **Ridership by day-of-week**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_t1 = df[['ENTRIESn_hourly', 'Hour']].groupby('Hour').sum()\n",
    "df_t1.index.name = 'Hour'\n",
    "df_t1.reset_index(inplace=True)\n",
    "\n",
    "df_t2 = df[['EXITSn_hourly', 'Hour']].groupby('Hour').sum()\n",
    "df_t2.index.name = 'Hour'\n",
    "df_t2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('Total entries and exits per hour of the day')\n",
    "plt.ylabel('Total')\n",
    "plt.xlabel('Hour of the day')\n",
    "plt.grid(True)\n",
    "plt.plot(df_t1['Hour'], df_t1['ENTRIESn_hourly'])\n",
    "plt.plot(df_t2['Hour'], df_t2['EXITSn_hourly'])\n",
    "plt.legend(['entries', 'exits'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Total entries and exits per hour of the day](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Quite interesting that `EXITS` are consistently smaller than `ENTRIES` : does NYC has subway stations with no turnstiles?\n",
    ">\n",
    ">Trying now ggplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_t1 = df[['ENTRIESn_hourly', 'EXITSn_hourly', 'DATEn']].groupby('DATEn').sum()\n",
    "df_t1.index.name = 'DATEn'\n",
    "df_t1.reset_index(inplace=True)\n",
    "df_t1['DATEn'] = pandas.to_datetime(df_t1['DATEn'])\n",
    "df_t1.head()\n",
    "\n",
    "df_t2 = pandas.melt(df1, 'DATEn')\n",
    "\n",
    "gg = ggplot(df_t2, aes(x='DATEn', y='value', colour = 'variable')) +\\\n",
    "    geom_line() +\\\n",
    "    ylab(\"Number entries or exits\") +\\\n",
    "    xlab(\"Day\") +\\\n",
    "    ggtitle(\"Total daily entries and exits\")\n",
    "print gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Total daily entries and exits](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Once more, it's possible to confirm a smaller amount of exits than entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4. Conclusion\n",
    "\n",
    "**Please address the following questions in detail. Your answers should be 1-2 paragraphs long.**\n",
    "\n",
    "**4.1 From your analysis and interpretation of the data, do more people ride\n",
    "the NYC subway when it is raining or when it is not raining?**\n",
    "\n",
    ">Yes, according the study here presented and based in the data used, it's possible to conclude with a high level of condifence that more people ride the NYC subway when it's raining than when it's not raining.\n",
    "\n",
    "**4.2 What analyses lead you to this conclusion? You should use results from both your statistical\n",
    "tests and your linear regression to support your analysis.**\n",
    "\n",
    ">In section 1, a full statistical analysis was presented on top of the given dataset. First, data was analyzed to verify what kind of statistical test could be used. Then, after observind data does not follow a normal distribution a non-parametrical test, Mann-Whitney U Test, was selected and applied. The resulting p-value leaded to reject the null hypothesis (stating no difference between rides when it's raining vs it's not raining) with high level of confidence `(2*p-value<0.05)`.\n",
    ">\n",
    ">Linear regression model using OLS (Statsmodels) on top of the given dataset has generated Rˆ2 lower than 0.5 - a not good result (but also not really bad). Residual histogram shows majority of residuals (original data - predicted) are around `0 +/- 5000`.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5. Reflection\n",
    "\n",
    "**Please address the following questions in detail. Your answers should be 1-2 paragraphs long.**\n",
    "\n",
    "**5.1 Please discuss potential shortcomings of the methods of your analysis, including:**\n",
    "- **Dataset,**\n",
    "- **Analysis, such as the linear regression model or statistical test.**\n",
    "\n",
    ">With respect to the data :\n",
    ">\n",
    ">1. **Data is from a single month : May/2011. This is a big issue for the analysis here made.** May is popular known as a 'nice wheater month' all around the globe, thus trying to figure out if people would ride more or less NYC subway using a single month does not seem reasonable. It would be better to have good data from all months during all seasons.\n",
    ">2. Further data analysis, verification and fixes may be required. For instance, by reading discussions at Udacity site, it's possible to find some potential flaws (like lot amount of entries in one hour followed by immediate almost nobody some minutes after. I haven't investigated in detail the data neither discussed or verified how the data was collected.\n",
    ">\n",
    ">With respect to the linear regression model :\n",
    ">\n",
    ">* The value for R2 obtained is 0.46 - not good but not bad... Trying other models (polynomial, logistic regreassion) may lead to better results.  \n",
    "\n",
    "---\n",
    "\n",
    "**5.2 (Optional) Do you have any other insight about the dataset that you would like to share with us?**\n",
    "\n",
    ">Most important comments about the dataset have been added in question above. While a great exercise (and I really enjoyed doing this project), the results in this study are useless as the data is from a single month. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in range(0,len(t_subs)):\n",
    "    if ((len(t_subs[k])==3) and ('Hour' in t_subs[k])):\n",
    "        print k, t_subs[k], t_rsqu[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(t_subs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_subs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'Hour' in t_subs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
