{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the NYC Subway Dataset\n",
    "\n",
    "Project connected to the [Udacity Intro to Data Science course](https://www.udacity.com/course/viewer#!/c-ud359-nd).\n",
    "\n",
    "by Victor Ribeiro, October/2015\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0. References\n",
    "\n",
    "**About the Dataset**\n",
    "\n",
    "Turnstile and Weather Variables dataset reports on the cumulative number of entries and exits in the NYC with additional information about the weather. \n",
    "\n",
    "* [Original Dataset](https://www.dropbox.com/s/meyki2wl9xfa7yk/turnstile_data_master_with_weather.csv) - data set used throughout the course and used in this report.\n",
    "* [Improved Dataset](https://www.dropbox.com/s/1lpoeh2w6px4diu/improved-dataset.zip?dl=0) - cleaned-up subset of original dataset with additional variables. [Variables in the dataset](https://s3.amazonaws.com/uploads.hipchat.com/23756/665149/05bgLZqSsMycnkg/turnstile-weather-variables.pdf)\n",
    "\n",
    "**References**\n",
    "\n",
    "* [Mann-Whitney U Test](https://storage.googleapis.com/supplemental_media/udacityu/4332539257/MannWhitneyUTest.pdf) Udacity\n",
    "* [Mann-Whitney U Test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test) Wikipedia\n",
    "* [Shapiro-Wilk Test](https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test) Wikipedia\n",
    "* [Shapiro-Wild Test](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html) Python reference\n",
    "* [Diez, David; Barr, Christopher; Çetinkaya-Rundel, Mine] OpenIntro Statistics, Third Edition\n",
    "* [Average monthly snow and rainfall in New York (millimeter)](https://weather-and-climate.com/average-monthly-precipitation-Rainfall,New-York,United-States-of-America)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1. Statistical Test\n",
    "\n",
    "**1.1 Which statistical test did you use to analyze the NYC subway data? Did you use a one-tail or a two-tail P value? What is the null hypothesis? What is your p-critical value?**\n",
    "\n",
    ">Considering the proposed project :\n",
    ">* Null hypothesis : there's no difference between number of rides in the metro while raining vs. no raining. \n",
    ">* Alternative hypothesis : there's a difference between number of rides in the metro while raining  vs.\n",
    "no raining.\n",
    ">\n",
    ">A Mann-Whitney U Test is applied. It's a two-tail test and p-critical value is 5% (or 0.05).\n",
    "\n",
    "---\n",
    "**1.2 Why is this statistical test applicable to the dataset? In particular, consider the assumptions that the test is making about the distribution of ridership in the two samples.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Taking a look in the data to support decision on 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandasql\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import csv\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "from ggplot import *\n",
    "import itertools\n",
    "\n",
    "df = pandas.read_csv(\"turnstile_data_master_with_weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bins = 25\n",
    "alpha = 0.75\n",
    "df[df['rain']==0]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "df[df['rain']==1]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "    \n",
    "plt.suptitle('Histogram of ENTRIESn_hourly')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('ENTRIESn_hourly')\n",
    "plt.legend(['no rain', 'rain'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram Raining and Non-Raining](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As per histogram above neither raining nor no-raining data follow a normal distribution. Indeed, by applying Shapiro-Wik test (below), we confirm datasets do not follow normal distribution as p-value for shapiro test on both raining / no-raining data is really small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.47661787271499634, 0.0)\n",
      "(0.4715914726257324, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print scipy.stats.shapiro(df[df['rain']==0]['ENTRIESn_hourly'])\n",
    "print scipy.stats.shapiro(df[df['rain']==1]['ENTRIESn_hourly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Thus, a non-parametric test (a test that does not assume the data is drawn from any particular underlying probability distribution) like Mann-Whithney U Test is applicable.\n",
    "\n",
    "---\n",
    "**1.3 What results did you get from this statistical test? These should include the following numerical values: p-values, as well as the means for each of the two samples under test.**\n",
    "\n",
    ">Applying Mann-Whitney U Test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mean with rain : 1105.44637675 \n",
      " Mean without rain : 1090.27878015 \n",
      " U : 1924409167.0 \n",
      " 2*p: 0.049999825587\n"
     ]
    }
   ],
   "source": [
    "with_rain_mean = np.mean(df[df['rain']==1]['ENTRIESn_hourly'])\n",
    "without_rain_mean = np.mean(df[df['rain']==0]['ENTRIESn_hourly'])\n",
    "    \n",
    "U,p = scipy.stats.mannwhitneyu(df[df['rain']==1]['ENTRIESn_hourly'],\n",
    "                               df[df['rain']==0]['ENTRIESn_hourly'])\n",
    "\n",
    "print ' Mean with rain :',with_rain_mean, \\\n",
    "      '\\n Mean without rain :', without_rain_mean, \\\n",
    "      '\\n U :', U, \\\n",
    "      '\\n 2*p:', 2*p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 What is the significance and interpretation of these results?**\n",
    "\n",
    ">`ENTRIESn_hourly` raining mean is slightly bigger than no-raining means. 2 \\* p-value is slightly below 0.05, thus **null hypothesis is rejected**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2. Linear Regression\n",
    "\n",
    "**2.1 What approach did you use to compute the coefficients theta and produce prediction for ENTRIESn_hourly in your regression model:**\n",
    "- **OLS using Statsmodels or Scikit Learn,**\n",
    "- **Gradient descent using Scikit Learn,**\n",
    "- **Or something different?**\n",
    "\n",
    ">OLS (using Statsmodels) has been selected to predict `ENTRIESn_hourly`. Below functions used to run linear regression and calculate R2. `predictions` is slightly different from the one I've done in class. The new version is more suitable for running a large number of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45837555079143655"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes r2 for a given dataset and its predictions\n",
    "def compute_r_squared(data, predictions):\n",
    "    n = ((data - predictions)**2).sum()\n",
    "    d = ((data - data.mean())**2).sum()\n",
    "    \n",
    "    r_squared = 1 - n/d\n",
    "    return r_squared\n",
    "\n",
    "# runs linear regression\n",
    "def linear_regression(features, values):\n",
    "    features = sm.add_constant(features)\n",
    "    model = sm.OLS(values,features)\n",
    "    results = model.fit()\n",
    "    intercept = results.params[0]\n",
    "    params = results.params[1:]    \n",
    "    \n",
    "    return intercept, params\n",
    "\n",
    "# given a dataframe and features, calculates predictions and params\n",
    "def predictions(dataframe, features):\n",
    "    # Values\n",
    "    values = dataframe['ENTRIESn_hourly']\n",
    "    # Perform linear regression\n",
    "    intercept, params = linear_regression(features, values)\n",
    "    predictions = intercept + np.dot(features, params)\n",
    "    \n",
    "    return predictions, params\n",
    "\n",
    "# a single run of linear regression with only one combination. Using :\n",
    "# Hour : as if it's late night or rush hour may change amount of rides\n",
    "# rain : as if it's raining people may take more the subway\n",
    "# precipi : same as above\n",
    "# meanwindspi : as if wind may lead people to take the subway\n",
    "# meantempi : as if it's really hot or cold, people may decide to take the subway\n",
    "# UNIT : as dummy variable\n",
    "features = df[['Hour','rain','precipi','meanwindspdi','meantempi']]\n",
    "dummy_units = pandas.get_dummies(df['UNIT'], prefix='unit')\n",
    "features = features.join(dummy_units)\n",
    "\n",
    "prediction, param = predictions(df, features)\n",
    "compute_r_squared(df['ENTRIESn_hourly'], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">After running multiple tests and obtaining very different results, I've decided to run some brute force to test a big number of different combinations. In order to decrease total amount of possible combinations, only means were considered for wind speedy, temperature, pressure and dewpti. All tests are including `UNIT` as dummy variable as it changes significantly R2 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List with almost all features. \n",
    "# Following guidance on exercise 5 Set 3, EXITSn_hourly is not being used.\n",
    "aall_features = ['Hour','precipi',\n",
    "                 'rain','fog', \n",
    "                 'meanwindspdi',\n",
    "                 'meantempi', \n",
    "                 'meanpressurei',\n",
    "                 'meandewpti' ]\n",
    "\n",
    "# multiple variables to log results\n",
    "i = 0\n",
    "l_rsqu = [] # log r2 for test i\n",
    "l_subs = [] # log for features subsets\n",
    "# global max logs and counter\n",
    "r_max = -1\n",
    "s_max = None \n",
    "i_max = 0\n",
    "para_max = None\n",
    "pred_max = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This brute force loop will select all combinations of features some specific sizes. \n",
    "for L in range(3,(len(aall_features)+1)):\n",
    "    # for each possible combination, runs a linear regression\n",
    "    for subset in itertools.combinations(aall_features, L):\n",
    "        l_rsqu.append(i)\n",
    "        l_subs.append(i)\n",
    "        l_subs[i] = subset\n",
    "        \n",
    "        # adding selected features and dummy variable\n",
    "        features = df[[subset[0]]]\n",
    "        for k in range(1,len(subset)):\n",
    "            features = features.join(df[[subset[k]]])\n",
    "        features = features.join(pandas.get_dummies(df['UNIT'], prefix='unit'))\n",
    "\n",
    "        # Perform linear regression\n",
    "        prediction, parameters = predictions(df, features)\n",
    "        l_rsqu[i] = compute_r_squared(df['ENTRIESn_hourly'], prediction)\n",
    "        \n",
    "        # Saving max\n",
    "        if r_max < l_rsqu[i]:\n",
    "            r_max = l_rsqu[i]\n",
    "            s_max = subset\n",
    "            i_max = i\n",
    "            para_max = parameters\n",
    "            pred_max = prediction\n",
    "            print '\\nNew max:', i_max, 'R2:', r_max, s_max\n",
    "        else:\n",
    "            print i, l_rsqu[i], subset\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">After running 219 combinations, R2 is max when using the combination with all 8 pre-selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " R2 Min :  0.418493719696 R2 Max :  0.458760123729 Ratio Min/Max :  0.912227759236\n"
     ]
    }
   ],
   "source": [
    "print 'R2 Min : ', min(l_rsqu), 'R2 Max : ', max(l_rsqu), \\\n",
    "      'Ratio Min/Max : ',min(l_rsqu)/max(l_rsqu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Which leaded me to run another test with all features and find another max for R2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4613875439198549"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df[['Hour', 'precipi', 'rain', 'fog','meanwindspdi',\n",
    "                'meantempi', 'meanpressurei', 'meandewpti',\n",
    "                'maxtempi', 'maxpressurei', 'mindewpti',\n",
    "                'mintempi', 'minpressurei', 'maxdewpti']]\n",
    "dummy_units = pandas.get_dummies(df['UNIT'], prefix='unit')\n",
    "features = features.join(dummy_units)\n",
    "\n",
    "prediction, param = predictions(df, features)\n",
    "r_4all = compute_r_squared(df['ENTRIESn_hourly'], prediction)\n",
    "r_4all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Important : while R2 is higher for the combination with all features, I will keep the combination with the highest R2 among all tested during the brute force tests.\n",
    ">\n",
    ">Now, the interesting part. Plotting R2 for all tests :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('Evolution of R2 for 219 tested combinations')\n",
    "plt.ylabel('R2 value')\n",
    "plt.xlabel('Test number')\n",
    "plt.grid(True)\n",
    "plt.plot(range(0,len(l_rsqu)), l_rsqu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![R2 for 219 tested combinations](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_5a.png?raw=true)\n",
    "\n",
    ">There is an interesting behavior on R2 accross the tests (chart above), it has basically two classes of values :\n",
    ">\n",
    ">- A higher one between 0.455 and 0.460 for some combinations \n",
    ">- A smaller one between 0.415 and 0.420 for other combinations\n",
    ">\n",
    "> This leaded me to check such combinations in search of what features are presented in each case. And it turns out that if `Hour` is present, R2 will jump to `0.455-0.460`. If `Hour` is not present, R2 will have values around `0.415-0.420`. Even, if all features are used but `Hour`, R2 will drop to around `0.420`.\n",
    ">\n",
    ">This is curious but makes sense as checking the chart in section 3.2 shows big jumps on rides during specific hours of the day - notably, hours related to going to / coming back from work/school and lunch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**2.2 What features (input variables) did you use in your model? Did you use any dummy variables as part of your features?**\n",
    "\n",
    ">Brute force was used to test multiple combinations. Features with the best result considering all 219 combinations were :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIT as dummy variable and : ('Hour', 'precipi', 'rain', 'fog', 'meanwindspdi', 'meantempi', 'meanpressurei', 'meandewpti')\n"
     ]
    }
   ],
   "source": [
    "print 'UNIT as dummy variable and :', s_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**2.3 Why did you select these features in your model? We are looking for specific reasons that lead you to believe that the selected features will contribute to the predictive power of your model.**\n",
    "- **Your reasons might be based on intuition. For example, response for fog might be: “I decided to use fog because I thought that when it is very foggy outside people might decide to use the subway more often.”**\n",
    "- **Your reasons might also be based on data exploration and experimentation, for example: “I used feature X because as soon as I included it in my model, it drastically improved my R2 value.”**\n",
    "\n",
    ">I did many tests with multiple combinations. An initial one was to pick the following features : \n",
    ">* Hour : as if it's late night or rush hours may change amount of rides (chart in 3.2)\n",
    ">* rain : as if it's raining people may take more the subway\n",
    ">* precipi : same as above\n",
    ">* meanwindspi : as if wind may lead people to take the subway\n",
    ">* meantempi : as if it's really hot or cold, people may decide to take the subway\n",
    ">* UNIT : as dummy variable\n",
    ">\n",
    ">Then, I've decided to run multiple combinations as well. To decrease total combinations possible, only means were considered for `meanwindspdi, meantempi, meanpressurei, meandewpti`. Interesting to notice that results don't change much among different combinations as seem on chart in section 2.1. \n",
    ">\n",
    ">The dummy variable 'UNIT' drastically improves R2 value. The feature `Hour` has high impact as well (section 2.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.4 What are the parameters (also known as \"coefficients\" or \"weights\") of the non-dummy features in your linear regression model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hour              67.399979\n",
       "precipi          -15.191443\n",
       "rain             -25.896405\n",
       "fog              103.996873\n",
       "meanwindspdi      23.564016\n",
       "meantempi         -4.401711\n",
       "meanpressurei   -208.144951\n",
       "meandewpti        -1.821180\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_max.head(len(s_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**2.5 What is your model’s R2 (coefficients of determination) value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4587601237285528"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.6 What does this R2 value mean for the goodness of fit for your regression model? Do you think this linear model to predict ridership is appropriate for this dataset, given this R2  value?**\n",
    "\n",
    "> R2 is the the percentage of variance that is explained. The closer R2 is to one, the better is the model. And, the closer to zero, the worse is the model. Our R2 is smaller than 0.5 (closer to zero than to one) which is mid-term, not good, but not bad. Below, a histogram of residuals (original data - predicted data) is presented. Most of the residuals are close to zero +/- 5000.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('Histogram of residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Difference original vs predicted')\n",
    "plt.grid(True)\n",
    "(df['ENTRIESn_hourly'] - pred_max).hist(bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram of residuals](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3. Visualization\n",
    "\n",
    "**Please include two visualizations that show the relationships between two or more variables in the NYC subway data.\n",
    "Remember to add appropriate titles and axes labels to your plots. Also, please add a short description below each figure commenting on the key insights depicted in the figure.**\n",
    "\n",
    "**3.1 One visualization should contain two histograms: one of  ENTRIESn_hourly for rainy days and one of ENTRIESn_hourly for non-rainy days.**\n",
    "- **You can combine the two histograms in a single plot or you can use two separate plots.**\n",
    "- **If you decide to use to two separate plots for the two histograms, please ensure that the x-axis limits for both of the plots are identical. It is much easier to compare the two in that case.**\n",
    "- **For the histograms, you should have intervals representing the volume of ridership (value of ENTRIESn_hourly) on the x-axis and the frequency of occurrence on the y-axis. For example, each interval (along the x-axis), the height of the bar for this interval will represent the number of records (rows in our data) that have ENTRIESn_hourly that falls in this interval.**\n",
    "- **Remember to increase the number of bins in the histogram (by having larger number of bars). The default bin width is not sufficient to capture the variability in the two samples.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "bins = 20\n",
    "alpha = 0.50\n",
    "df[df['rain']==0]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "df[df['rain']==1]['ENTRIESn_hourly'].hist(bins = bins, alpha=alpha) \n",
    "    \n",
    "plt.suptitle('Histogram of ENTRIESn_hourly')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('ENTRIESn_hourly')\n",
    "plt.legend(['no rain', 'rain'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Histogram Raining and Non-Raining](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**3.2 One visualization can be more freeform. You should feel free to implement something that we discussed in class (e.g., scatter plots, line plots) or attempt to implement something more advanced if you'd like. Some suggestions are:**\n",
    "- **Ridership by time-of-day**\n",
    "- **Ridership by day-of-week**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_t1 = df[['ENTRIESn_hourly', 'Hour']].groupby('Hour').sum()\n",
    "df_t1.index.name = 'Hour'\n",
    "df_t1.reset_index(inplace=True)\n",
    "\n",
    "df_t2 = df[['EXITSn_hourly', 'Hour']].groupby('Hour').sum()\n",
    "df_t2.index.name = 'Hour'\n",
    "df_t2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.suptitle('Total entries and exits per hour of the day')\n",
    "plt.ylabel('Total')\n",
    "plt.xlabel('Hour of the day')\n",
    "plt.grid(True)\n",
    "plt.plot(df_t1['Hour'], df_t1['ENTRIESn_hourly'])\n",
    "plt.plot(df_t2['Hour'], df_t2['EXITSn_hourly'])\n",
    "plt.legend(['entries', 'exits'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Total entries and exits per hour of the day](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Quite interesting that `EXITS` are consistently smaller than `ENTRIES` : does NYC has subway stations with no turnstiles?\n",
    ">\n",
    ">Using ggplot for another chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ggplot: (298715789)>\n"
     ]
    }
   ],
   "source": [
    "df_t1 = df[['ENTRIESn_hourly', 'EXITSn_hourly', 'DATEn']].groupby('DATEn').sum()\n",
    "df_t1.index.name = 'DATEn'\n",
    "df_t1.reset_index(inplace=True)\n",
    "df_t1['DATEn'] = pandas.to_datetime(df_t1['DATEn'])\n",
    "df_t1.head()\n",
    "\n",
    "df_t2 = pandas.melt(df_t1, 'DATEn')\n",
    "\n",
    "gg = ggplot(df_t2, aes(x='DATEn', y='value', colour = 'variable')) +\\\n",
    "    geom_line() +\\\n",
    "    ylab('Number entries or exits') +\\\n",
    "    xlab('') +\\\n",
    "    ggtitle('Total daily entries and exits') +\\\n",
    "    theme(axis_text_x = element_text(size=8,angle=45),\n",
    "          axis_text_y = element_text(size=8))\n",
    "print gg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Total daily entries and exits](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_4a.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Once more, it's possible to confirm a smaller amount of exits than entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4. Conclusion\n",
    "\n",
    "**Please address the following questions in detail. Your answers should be 1-2 paragraphs long.**\n",
    "\n",
    "**4.1 From your analysis and interpretation of the data, do more people ride\n",
    "the NYC subway when it is raining or when it is not raining?**\n",
    "\n",
    ">According my analysis, it's possible to conclude with high level of condifence that the number of entries in the NYC subway is statistically different bettween rainy an non rainy days. A one-tailed test should be done to conclude if more or less people ride the NYC subway in rainy than on non rainy days.\n",
    "\n",
    "**4.2 What analyses lead you to this conclusion? You should use results from both your statistical\n",
    "tests and your linear regression to support your analysis.**\n",
    "\n",
    ">In section 1, a full statistical analysis was presented on top of the given dataset. First, data was analyzed to verify what kind of statistical test could be used. Then, after validating that data does not follow a normal distribution, a non-parametrical test (Mann-Whitney U Test) was selected and applied. The resulting p-value leaded to reject the null hypothesis (stating no difference between rides when it's raining vs when it's not raining) with high level of confidence `(2*p-value<0.05)`.\n",
    ">\n",
    ">In section 2.1, a linear regression model OLS (Statsmodels) was applied on top of the given dataset. After some tests, including running brute force for 219 different type of features combinations, R2 reached max value around `0.455-0.460`. A dummy variable `UNIT` was critical to reach such values, as well as the feature `Hour`. Curiously, `rain` and `precipi` are presented as features in the test with highest R2, but both have negative parameters (section 2.4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5. Reflection\n",
    "\n",
    "**Please address the following questions in detail. Your answers should be 1-2 paragraphs long.**\n",
    "\n",
    "**5.1 Please discuss potential shortcomings of the methods of your analysis, including:**\n",
    "- **Dataset,**\n",
    "- **Analysis, such as the linear regression model or statistical test.**\n",
    "\n",
    ">With respect to the data :\n",
    ">\n",
    ">1. **Data is from a single month : May/2011. This is a big issue for the analysis here made.** Using a single month does not seem reasonable due to seasonal effects. It would be better to have good data from all months during all seasons. ![Total daily entries and exits](https://github.com/vfribeiro/IntroDataScience/blob/master/figure_6.png?raw=true)\n",
    ">2. Further data analysis, verification and fixes may be required. For instance, chart in 3.2 shows a big difference between amount of entries and exits.\n",
    ">\n",
    ">With respect to the linear regression model and statistical test :\n",
    ">\n",
    ">- The value for R2 obtained is between `0.455-0.460` - not good but not bad... Trying other models (polynomial, logistic regreassion plus regularization) may lead to better results.\n",
    ">- There are other non-parametrical tests. Running additional ones would be useful to double check.\n",
    "\n",
    "---\n",
    "\n",
    "**5.2 (Optional) Do you have any other insight about the dataset that you would like to share with us?**\n",
    "\n",
    ">Most important comments about the dataset have been added in question above. \n",
    ">\n",
    ">Also, one should think a lot about the question made. The investigation here was interesting, but it seems that analysis more focused on `Hour` of the day could lead to more interesting conclusions in order to help the NYC Subway.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
